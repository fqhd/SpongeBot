{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1516547,"sourceType":"datasetVersion","datasetId":893744}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom gensim.models import KeyedVectors\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nfrom collections import Counter\nimport random\nimport torch.nn.functional as F\nimport os\nimport json\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:13:23.694215Z","iopub.execute_input":"2025-06-15T11:13:23.694475Z","iopub.status.idle":"2025-06-15T11:13:23.698786Z","shell.execute_reply.started":"2025-06-15T11:13:23.694457Z","shell.execute_reply":"2025-06-15T11:13:23.698021Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"!curl 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip' --output 'wiki-news-300d-1M.vec.zip'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:07:51.372810Z","iopub.execute_input":"2025-06-15T09:07:51.373641Z","iopub.status.idle":"2025-06-15T09:07:54.189054Z","shell.execute_reply.started":"2025-06-15T09:07:51.373614Z","shell.execute_reply":"2025-06-15T09:07:54.188253Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  650M  100  650M    0     0   251M      0  0:00:02  0:00:02 --:--:--  251M\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!unzip 'wiki-news-300d-1M.vec.zip'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:07:54.190886Z","iopub.execute_input":"2025-06-15T09:07:54.191152Z","iopub.status.idle":"2025-06-15T09:08:12.194594Z","shell.execute_reply.started":"2025-06-15T09:07:54.191128Z","shell.execute_reply":"2025-06-15T09:08:12.193868Z"}},"outputs":[{"name":"stdout","text":"Archive:  wiki-news-300d-1M.vec.zip\n  inflating: wiki-news-300d-1M.vec   \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"fasttext = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', binary=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:08:12.195555Z","iopub.execute_input":"2025-06-15T09:08:12.195804Z","iopub.status.idle":"2025-06-15T09:10:50.374500Z","shell.execute_reply.started":"2025-06-15T09:08:12.195773Z","shell.execute_reply":"2025-06-15T09:10:50.373912Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"word_vec = fasttext['spongebob']\npunct_vec = fasttext['!']\n\nprint(\"Vector for 'spongebob':\", word_vec[:5])\nprint(\"Vector for '!':\", punct_vec[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:50.376457Z","iopub.execute_input":"2025-06-15T09:10:50.376740Z","iopub.status.idle":"2025-06-15T09:10:50.382218Z","shell.execute_reply.started":"2025-06-15T09:10:50.376721Z","shell.execute_reply":"2025-06-15T09:10:50.381590Z"}},"outputs":[{"name":"stdout","text":"Vector for 'spongebob': [-0.0727 -0.0882 -0.2449 -0.0302 -0.1174]\nVector for '!': [-0.1894 -0.002  -0.0817  0.0334  0.1775]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def load_text():\n    text = ''\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            with open(os.path.join(dirname, filename)) as f:\n                text += f.read()\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:50.383041Z","iopub.execute_input":"2025-06-15T09:10:50.383271Z","iopub.status.idle":"2025-06-15T09:10:50.401072Z","shell.execute_reply.started":"2025-06-15T09:10:50.383246Z","shell.execute_reply":"2025-06-15T09:10:50.400240Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def tokenize(text):\n    tokens = []\n    current = \"\"\n\n    for char in text:\n        if char.isalpha():\n            current += char\n        else:\n            if current:\n                tokens.append(current)\n                current = \"\"\n            if char == '\\n':\n                tokens.append('\\n')       # newline is its own token\n            elif char.strip() == \"\":\n                tokens.append(' ')        # space or tab\n            else:\n                tokens.append(char)       # punctuation\n\n    if current:\n        tokens.append(current)\n\n    return tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:50.402052Z","iopub.execute_input":"2025-06-15T09:10:50.402577Z","iopub.status.idle":"2025-06-15T09:10:50.421910Z","shell.execute_reply.started":"2025-06-15T09:10:50.402552Z","shell.execute_reply":"2025-06-15T09:10:50.421165Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print(tokenize('Hello, this is a test\\'n to see if the \\ntokenize function works! buuuut we will have to see... so, i hope it works :) but it\\'s going to depend on it for sure! Anyway. We will see, I\\'ll see, haha!'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:50.422679Z","iopub.execute_input":"2025-06-15T09:10:50.422950Z","iopub.status.idle":"2025-06-15T09:10:50.436954Z","shell.execute_reply.started":"2025-06-15T09:10:50.422928Z","shell.execute_reply":"2025-06-15T09:10:50.436137Z"}},"outputs":[{"name":"stdout","text":"['Hello', ',', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test', \"'\", 'n', ' ', 'to', ' ', 'see', ' ', 'if', ' ', 'the', ' ', '\\n', 'tokenize', ' ', 'function', ' ', 'works', '!', ' ', 'buuuut', ' ', 'we', ' ', 'will', ' ', 'have', ' ', 'to', ' ', 'see', '.', '.', '.', ' ', 'so', ',', ' ', 'i', ' ', 'hope', ' ', 'it', ' ', 'works', ' ', ':', ')', ' ', 'but', ' ', 'it', \"'\", 's', ' ', 'going', ' ', 'to', ' ', 'depend', ' ', 'on', ' ', 'it', ' ', 'for', ' ', 'sure', '!', ' ', 'Anyway', '.', ' ', 'We', ' ', 'will', ' ', 'see', ',', ' ', 'I', \"'\", 'll', ' ', 'see', ',', ' ', 'haha', '!']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def build_vocab(tokens, max_vocab_size=1024):\n    # Count token frequencies\n    token_counts = Counter(tokens)\n    # Most common tokens up to max_vocab_size - 1 (reserve 1 for <UNK>)\n    most_common = token_counts.most_common(max_vocab_size - 1)\n    # Build vocab dict: token -> index, reserve 0 for <UNK>\n    vocab = {token: idx + 1 for idx, (token, _) in enumerate(most_common)}\n    vocab['<UNK>'] = 0\n    return vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:50.437683Z","iopub.execute_input":"2025-06-15T09:10:50.437931Z","iopub.status.idle":"2025-06-15T09:10:50.452579Z","shell.execute_reply.started":"2025-06-15T09:10:50.437913Z","shell.execute_reply":"2025-06-15T09:10:50.451862Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def tokens_to_indices(tokens, vocab):\n    # Map tokens to indices; use 0 (<UNK>) if token not in vocab\n    return [vocab.get(token, 0) for token in tokens]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:50.453351Z","iopub.execute_input":"2025-06-15T09:10:50.453611Z","iopub.status.idle":"2025-06-15T09:10:50.470217Z","shell.execute_reply.started":"2025-06-15T09:10:50.453591Z","shell.execute_reply":"2025-06-15T09:10:50.469433Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print('Loading Spongebob Transcript...')\ntext = load_text()\nprint('Finished Loading!')\nprint(len(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:50.472912Z","iopub.execute_input":"2025-06-15T09:10:50.473180Z","iopub.status.idle":"2025-06-15T09:10:51.929842Z","shell.execute_reply.started":"2025-06-15T09:10:50.473164Z","shell.execute_reply":"2025-06-15T09:10:51.929060Z"}},"outputs":[{"name":"stdout","text":"Loading Spongebob Transcript...\nFinished Loading!\n5082524\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def clean_ascii(text):\n    return ''.join(c for c in text if ord(c) < 128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:51.930786Z","iopub.execute_input":"2025-06-15T09:10:51.931372Z","iopub.status.idle":"2025-06-15T09:10:51.935014Z","shell.execute_reply.started":"2025-06-15T09:10:51.931343Z","shell.execute_reply":"2025-06-15T09:10:51.934445Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"tokens = tokenize(clean_ascii(text.lower()))\nprint(len(tokens))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:51.936145Z","iopub.execute_input":"2025-06-15T09:10:51.936405Z","iopub.status.idle":"2025-06-15T09:10:52.775117Z","shell.execute_reply.started":"2025-06-15T09:10:51.936381Z","shell.execute_reply":"2025-06-15T09:10:52.774363Z"}},"outputs":[{"name":"stdout","text":"2127632\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"vocab = build_vocab(tokens, max_vocab_size=4096)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:52.776043Z","iopub.execute_input":"2025-06-15T09:10:52.776336Z","iopub.status.idle":"2025-06-15T09:10:53.006336Z","shell.execute_reply.started":"2025-06-15T09:10:52.776313Z","shell.execute_reply":"2025-06-15T09:10:53.005082Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"with open('vocab.json', 'w') as f:\n    json.dump(vocab, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:53.007353Z","iopub.execute_input":"2025-06-15T09:10:53.007754Z","iopub.status.idle":"2025-06-15T09:10:53.017735Z","shell.execute_reply.started":"2025-06-15T09:10:53.007724Z","shell.execute_reply":"2025-06-15T09:10:53.016940Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"indices = tokens_to_indices(tokens, vocab)\nprint(indices[100:200])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:53.018723Z","iopub.execute_input":"2025-06-15T09:10:53.019674Z","iopub.status.idle":"2025-06-15T09:10:53.176480Z","shell.execute_reply.started":"2025-06-15T09:10:53.019645Z","shell.execute_reply":"2025-06-15T09:10:53.175754Z"}},"outputs":[{"name":"stdout","text":"[519, 1, 6, 1, 4040, 2, 4, 7, 382, 1, 27, 1, 314, 1, 3308, 1, 25, 1, 591, 1, 12, 1, 2133, 1, 2084, 1, 27, 1, 589, 1, 13, 1, 0, 1, 857, 1, 14, 1, 118, 1, 3127, 2, 8, 4, 2804, 1, 143, 5, 1, 7, 1076, 1, 6, 1, 4040, 1, 1681, 1, 12, 1, 1149, 1, 56, 1, 13, 1, 389, 8, 1, 658, 31, 650, 10, 1, 7, 230, 1, 28, 1, 6, 1, 4040, 1, 389, 1, 12, 1, 138, 1, 14, 1, 491, 8, 4, 7, 6, 1, 87, 1, 138]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def coverage(tokens, vocab):\n    known = sum(1 for t in tokens if t in vocab)\n    return known / len(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:53.177280Z","iopub.execute_input":"2025-06-15T09:10:53.177577Z","iopub.status.idle":"2025-06-15T09:10:53.181757Z","shell.execute_reply.started":"2025-06-15T09:10:53.177548Z","shell.execute_reply":"2025-06-15T09:10:53.180908Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"coverage(tokens, vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:10:53.182394Z","iopub.execute_input":"2025-06-15T09:10:53.182626Z","iopub.status.idle":"2025-06-15T09:10:53.331214Z","shell.execute_reply.started":"2025-06-15T09:10:53.182609Z","shell.execute_reply":"2025-06-15T09:10:53.330394Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"0.9779882047271332"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"class LSTMTextGen(nn.Module):\n    def __init__(self, embedding_matrix, hidden_size=1024, num_layers=4, dropout=0.3):\n        super().__init__()\n        vocab_size, embedding_dim = embedding_matrix.shape\n        \n        self.embedding = nn.Embedding.from_pretrained(\n            torch.tensor(embedding_matrix, dtype=torch.float32),\n            freeze=False  # allow training embeddings\n        )\n\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout=dropout,\n            batch_first=True\n        )\n\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, hidden=None):\n        x = self.embedding(x)\n        output, hidden = self.lstm(x, hidden)\n        output = self.layer_norm(output)\n        logits = self.fc(output)\n        return logits, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:05:27.207097Z","iopub.execute_input":"2025-06-15T11:05:27.207820Z","iopub.status.idle":"2025-06-15T11:05:27.216149Z","shell.execute_reply.started":"2025-06-15T11:05:27.207786Z","shell.execute_reply":"2025-06-15T11:05:27.215477Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class LSTMTextGen(nn.Module):\n    def __init__(self, embedding_matrix, hidden_size=2048, num_layers=2):\n        super().__init__()\n        vocab_size, embedding_dim = embedding_matrix.shape\n        \n        self.embedding = nn.Embedding.from_pretrained(\n            torch.tensor(embedding_matrix, dtype=torch.float32),\n            freeze=False  # allow training embeddings\n        )\n\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True\n        )\n\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, hidden=None):\n        x = self.embedding(x)\n        output, hidden = self.lstm(x, hidden)\n        output = self.layer_norm(output)\n        logits = self.fc(output)\n        return logits, hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:37:54.002804Z","iopub.execute_input":"2025-06-15T11:37:54.003355Z","iopub.status.idle":"2025-06-15T11:37:54.009009Z","shell.execute_reply.started":"2025-06-15T11:37:54.003334Z","shell.execute_reply":"2025-06-15T11:37:54.008215Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:37:55.402760Z","iopub.execute_input":"2025-06-15T11:37:55.403374Z","iopub.status.idle":"2025-06-15T11:37:55.407294Z","shell.execute_reply.started":"2025-06-15T11:37:55.403353Z","shell.execute_reply":"2025-06-15T11:37:55.406616Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"def build_embedding_matrix(vocab, fasttext, embedding_dim=300):\n    matrix = np.zeros((len(vocab), embedding_dim), dtype=np.float32)\n    \n    for token, idx in vocab.items():\n        if token in fasttext:\n            matrix[idx] = fasttext[token]\n        else:\n            # Random init for <UNK> or missing tokens\n            matrix[idx] = np.random.normal(scale=0.6, size=embedding_dim)\n    \n    return matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:37:55.615944Z","iopub.execute_input":"2025-06-15T11:37:55.616384Z","iopub.status.idle":"2025-06-15T11:37:55.620727Z","shell.execute_reply.started":"2025-06-15T11:37:55.616362Z","shell.execute_reply":"2025-06-15T11:37:55.620026Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"embedding_matrix = build_embedding_matrix(vocab, fasttext, embedding_dim=300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:15:14.997115Z","iopub.execute_input":"2025-06-15T11:15:14.997358Z","iopub.status.idle":"2025-06-15T11:15:15.012787Z","shell.execute_reply.started":"2025-06-15T11:15:14.997342Z","shell.execute_reply":"2025-06-15T11:15:15.011851Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"model = LSTMTextGen(embedding_matrix).to(device)\n\nprint(sum([0 if param.requires_grad == False else param.numel() for param in model.parameters()]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:37:59.097191Z","iopub.execute_input":"2025-06-15T11:37:59.097701Z","iopub.status.idle":"2025-06-15T11:37:59.658962Z","shell.execute_reply.started":"2025-06-15T11:37:59.097680Z","shell.execute_reply":"2025-06-15T11:37:59.658245Z"}},"outputs":[{"name":"stdout","text":"62447616\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"def get_batch(seq_len=32, batch_size=64):\n    inputs = torch.empty((batch_size, seq_len), dtype=torch.long)\n    targets = torch.empty((batch_size, seq_len), dtype=torch.long)\n\n    max_start = len(indices) - seq_len - 1\n    for i in range(batch_size):\n        start = random.randint(0, max_start)\n        seq = indices[start:start + seq_len]\n        tgt = indices[start + 1:start + seq_len + 1]\n\n        inputs[i] = torch.tensor(seq, dtype=torch.long)\n        targets[i] = torch.tensor(tgt, dtype=torch.long)\n\n    return inputs, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:15:16.917884Z","iopub.execute_input":"2025-06-15T11:15:16.918152Z","iopub.status.idle":"2025-06-15T11:15:16.923581Z","shell.execute_reply.started":"2025-06-15T11:15:16.918131Z","shell.execute_reply":"2025-06-15T11:15:16.922844Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def train(model, num_iters=1000, seq_len=64, batch_size=256, lr=1e-3):\n    model.train()\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    losses = []\n\n    for step in range(1, num_iters + 1):\n        x, y = get_batch(seq_len, batch_size)\n        x, y = x.to(device), y.to(device)\n\n        optimizer.zero_grad()\n        logits, _ = model(x)\n\n        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n        if step % 100 == 0 or step == 1:\n            avg_loss = sum(losses) / len(losses)\n            losses = []\n            print(f\"Step {step}/{num_iters} | Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:38:37.257699Z","iopub.execute_input":"2025-06-15T11:38:37.258331Z","iopub.status.idle":"2025-06-15T11:38:37.263944Z","shell.execute_reply.started":"2025-06-15T11:38:37.258306Z","shell.execute_reply":"2025-06-15T11:38:37.263196Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"for _ in range(10):\n    train(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:38:39.733823Z","iopub.execute_input":"2025-06-15T11:38:39.734392Z"}},"outputs":[{"name":"stdout","text":"Step 1/1000 | Loss: 8.3848\nStep 100/1000 | Loss: 3.7935\nStep 200/1000 | Loss: 3.0937\nStep 300/1000 | Loss: 2.7840\nStep 400/1000 | Loss: 2.5832\nStep 500/1000 | Loss: 2.4700\nStep 600/1000 | Loss: 2.3865\nStep 700/1000 | Loss: 2.3157\nStep 800/1000 | Loss: 2.2538\nStep 900/1000 | Loss: 2.2042\nStep 1000/1000 | Loss: 2.1564\nStep 1/1000 | Loss: 2.1574\nStep 100/1000 | Loss: 2.1562\nStep 200/1000 | Loss: 2.0782\nStep 300/1000 | Loss: 2.0414\nStep 400/1000 | Loss: 2.0029\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"index_to_token = {idx: token for token, idx in vocab.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:02:27.211226Z","iopub.execute_input":"2025-06-15T11:02:27.211789Z","iopub.status.idle":"2025-06-15T11:02:27.216373Z","shell.execute_reply.started":"2025-06-15T11:02:27.211763Z","shell.execute_reply":"2025-06-15T11:02:27.215857Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"with open('index_to_token.json', 'w') as f:\n    json.dump(index_to_token, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:02:28.067140Z","iopub.execute_input":"2025-06-15T11:02:28.067385Z","iopub.status.idle":"2025-06-15T11:02:28.075999Z","shell.execute_reply.started":"2025-06-15T11:02:28.067365Z","shell.execute_reply":"2025-06-15T11:02:28.075284Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def generate(model, input_sequence, length, temperature=1.0):\n    model.eval()\n\n    sentence = ''\n\n    tokens = tokenize(input_sequence.lower())\n\n    indices = tokens_to_indices(tokens, vocab)\n\n    hidden = None\n\n    for idx in indices:\n        inp = torch.tensor([[idx]], dtype=torch.long).to(device)\n        _, hidden = model(inp, hidden)\n\n    inp = torch.tensor([[indices[-1]]], dtype=torch.long).to(device)\n\n    for _ in range(length):\n        logits, hidden = model(inp, hidden)  # logits: (1, 1, vocab_size)\n        logits = logits[:, -1, :] / temperature\n        logits[0, vocab['<UNK>']] = -float('inf') # Make <UNK> impossible to sample\n        probs = F.softmax(logits, dim=-1)\n\n        next_idx = torch.multinomial(probs, num_samples=1).item()\n\n        sentence += index_to_token[next_idx]\n        \n        inp = torch.tensor([[next_idx]], dtype=torch.long).to(device)\n\n    return input_sequence + sentence\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:02:39.843407Z","iopub.execute_input":"2025-06-15T11:02:39.844069Z","iopub.status.idle":"2025-06-15T11:02:39.849615Z","shell.execute_reply.started":"2025-06-15T11:02:39.844046Z","shell.execute_reply":"2025-06-15T11:02:39.849044Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"print(generate(model, 'Patrick: Hi Spongebob, How are you doing?\\nSpongebob: ', 250))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:06:19.257181Z","iopub.execute_input":"2025-06-15T11:06:19.257950Z","iopub.status.idle":"2025-06-15T11:06:19.529342Z","shell.execute_reply.started":"2025-06-15T11:06:19.257921Z","shell.execute_reply":"2025-06-15T11:06:19.528560Z"}},"outputs":[{"name":"stdout","text":"Patrick: Hi Spongebob, How are you doing?\nSpongebob: spongebobwalterknocksatollantennaflashmetpersonstoppingthendecidecoloryupgrease6butterflytrulyimaginesecondssureinstantgroundtowardkisshoplovestightduerushes4ahoyfranticallycarolgangboredchargescarfishshapemassivemmraindoocompletelywhateverhistorycoveredhockeyallmissingupsidemayorslugfishcopseriouslypetersonnewestsetstrainsnancystrugglingscratchesmealssetbillysalesmanweekminiebellgoessamplewhitecrashesmiserygreekspatulassisterestablishmentoutprivateworsedisappointedpoplongtibutterflypaintyboardsshakesomebodypaintedmicrowavecakepipeinterruptsroundrexboltsthankturningpunchuponsnapsimpressedspendinglellyacornspeanutmagirlsbeatentestnowhereshutwrittenkelpshakekelpshakegreen2angrilywonderfulfoolsfoolingcardeelssmashmisterfergusonascranebookstreedomelageniussmittyshuddershandyayearstraightunnamedpreciousbooroyalcongratulationsyewalleatenswabsuburbanslitherfuzzydownwarddrumsimpressedpopprizedrumsugheyeposterspringarrivecominpecosperkinsquitmissingnewsbashesnumbersdrivermostprofessorwrestlersfoolsmint$snowballsnatchesreappearsourselvesobviouscolorfulstarsletbravostuffedtableheymadenamesdroolwowboatmobileropespidersqueezesfilthyticklesfamilydreamsnoonservicereleasessofanotescakefeetblahmmmchefbobminuteusedtwirlsbathtubmoonfightmouthbrilliantrampcarolwigwhooamazinglamebuyorderingmayonnaisemountaineelscowboycombpassedcompanypeelsnoisesropesnappapaequipmentsplitsblenderpoemworststuffssettingboilingmalleyexcitement\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"model = model.cpu()\ntorch.save(model.state_dict(), 'spongebob_lstm_27M.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T11:04:00.098696Z","iopub.execute_input":"2025-06-15T11:04:00.099212Z","iopub.status.idle":"2025-06-15T11:04:00.427775Z","shell.execute_reply.started":"2025-06-15T11:04:00.099188Z","shell.execute_reply":"2025-06-15T11:04:00.426984Z"}},"outputs":[],"execution_count":40}]}